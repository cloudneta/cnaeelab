---
layout: post
title: 3장 실습 Amazon EKS - LGTM Observability Full Stack 구성하기
subtitle: EKS 환경에서 LGTM Observability Full Stack 환경을 구성하고 확인합니다.
tags: [eks, 3장]
published: true
---
|목차|
|-----------|
|[1. 기본 환경 배포](#1-기본-환경-배포)|
|[&nbsp;&nbsp;&nbsp;&nbsp;1.1. Terraform을 통한 기본 인프라 배포](#11-terraform을-통한-기본-인프라-배포)|
|[&nbsp;&nbsp;&nbsp;&nbsp;1.2. 기본 정보 확인 및 설정](#12-기본-정보-확인-및-설정)|
|[2. Observability Backends 설치](#2-observability-backends-설치)|
|[&nbsp;&nbsp;&nbsp;&nbsp;2.1. Grafana Mimir 설치 및 확인](#21-grafana-mimir-설치-및-확인)|
|[&nbsp;&nbsp;&nbsp;&nbsp;2.2. Grafana Loki 설치 및 확인](#22-grafana-loki-설치-및-확인)|
|[&nbsp;&nbsp;&nbsp;&nbsp;2.3. Grafana Tempo 설치 및 확인](#23-grafana-tempo-설치-및-확인)|
|[3. OpenTelemetry Operator & Collector 설치](#3-opentelemetry-operator--collector-설치)|
|[&nbsp;&nbsp;&nbsp;&nbsp;3.1. OpenTelemetry Operator 설치](#31-opentelemetry-operator-설치)|
|[&nbsp;&nbsp;&nbsp;&nbsp;3.2. OpenTelemetry Collector 생성](#32-opentelemetry-collector-생성)|
|[4. Observability Visualization 설치](#4-observability-visualization-설치)|
|[&nbsp;&nbsp;&nbsp;&nbsp;4.1. Grafana 설치](#41-grafana-설치)|
|[5. LGTM Observability 검증](#5-lgtm-observability-검증)|
|[&nbsp;&nbsp;&nbsp;&nbsp;5.1. [Metrics] Grafana Mimir 확인](#51-metrics-grafana-mimir-확인)|
|[&nbsp;&nbsp;&nbsp;&nbsp;5.2. [Logs] Grafana Loki 확인](#52-logs-grafana-loki-확인)|
|[&nbsp;&nbsp;&nbsp;&nbsp;5.3. [Traces] Grafana Tempo 확인](#53-traces-grafana-tempo-확인)|
|[6. 실습 환경 삭제](#6-실습-환경-삭제)|
|[&nbsp;&nbsp;&nbsp;&nbsp;6.1. 실습 자원 삭제](#61-실습-자원-삭제)|
|[&nbsp;&nbsp;&nbsp;&nbsp;6.2. Terraform 삭제](#62-terraform-삭제)|

<br/>


## 1. 기본 환경 배포

<br/>

이번 실습은 IAM 사용자 계정을 통해 관리 콘솔에 접근하고 액세스 키를 활용해 awscli 도구를 사용합니다.  
해당 작업을 수행하지 않았다면 아래 토글을 확장해 작업을 선행하고 본격적인 실습에 들어갑니다.

<details>
<summary><span style='color:orange'>IAM 사용자 생성 및 액세스 키 생성</span></summary>
<div markdown="1">

<br/>

<span style='color:white; background-color:#404040'> **IAM 사용자 생성** </span>

- 루트 계정으로 로그인하여 [링크](https://us-east-1.console.aws.amazon.com/iamv2/home#/users){:target="_blank"}에 클릭하여 IAM 사용자 페이지에 진입합니다.
- `사용자 추가` 버튼을 클릭합니다.
- <U>사용자 이름</U>은 *admin*으로 입력하고 [AWS Management Console에 대한 사용자 액세스 권한 제공]을 체크합니다.
- <U>사용자에게 콘솔 액세스 권한 제공</U>은 [IAM 사용자를 생성하고 싶음]을 선택합니다.
- <U>콘솔 암호</U>는 [사용자 지정 암호]를 선택하고 생성 기준에 맞춰 각자 암호를 지정합니다.
- <U>사용자는 다음 로그인 시 새 암호를 생성해야 합니다.</U>를 체크 해제하고 `다음` 버튼을 클릭합니다.
- <U>권한 옵션</U>은 [직접 정책 연결]을 선택하고 <U>권한 정책</U>에서 [AdministratorAccess]를 체크한 후 아래 `다음` 버튼을 클릭합니다.
- 검토 및 생성 페이지에서 `사용자 생성` 버튼을 클릭합니다.
- 암호 검색 페이지에서 `.csv 파일 다운로드` 버튼을 클릭하여 자신의 PC의 디렉터리에 저장합니다.
- `사용자 목록으로 돌아가기` 버튼을 클릭하여 IAM 사용자 생성을 마무리합니다.

<br/>

<span style='color:white; background-color:#404040'> **IAM 사용자 액세스 키 생성** </span>

- IAM 사용자 페이지에서 `생성한 사용자 이름`을 클릭합니다.
- `보안 자격 증명` 탭을 클릭하고 [액세스 키] 영역에서 `액세스 키 만들기` 버튼을 클릭합니다.
- 액세스 키 모범 사례 및 대안 페이지에서 [Command Line Interface(CLI)]를 선택하고 아래 체크 박스를 체크한 후 `다음` 버튼을 클릭합니다.
- `액세스 키 만들기` 버튼을 클릭합니다.
- 액세스 키 검색 페이지에서 `.csv 파일 다운로드` 버튼을 클릭하여 자신의 PC의 디렉터리에 저장합니다.
- `완료` 버튼을 클릭하여 IAM 사용자 액세스 키 생성을 마무리합니다.

{: .box-note}
**Note:** IAM 사용자로 관리 콘솔에 로그인 할때 계정 ID가 필요하니 잘 메모해 둡니다.

</div>
</details>


<br/>

### 1.1. Terraform을 통한 기본 인프라 배포

Terraform을 통한 기본 인프라 배포에 앞서 SSH 키 페어, IAM User Access Key ID, IAM User Secret Access Key를 미리 확인하고 메모해 둡니다.

<br/>

<span style='color:white; background-color:#404040'> **Terraform으로 기본 인프라 배포** </span>

```
## 실습 디렉터리 경로 진입
cd cnaee_class_tf/ch3
  
  
## Terraform 환경 변수 저장
export TF_VAR_KeyName=[각자 ssh keypair]
export TF_VAR_MyIamUserAccessKeyID=[각자 iam 사용자의 access key id]
export TF_VAR_MyIamUserSecretAccessKey=[각자 iam 사용자의 secret access key]
export TF_VAR_SgIngressSshCidr=$(curl -s ipinfo.io/ip)/32
  
  
## Terraform 배포
terraform init

terraform plan

terraform apply -auto-approve
```

<br/>

Terraform을 통한 기본 인프라 배포가 완료되면 관리 콘솔에서 생성된 인프라들을 확인합니다.

{: .box-note}
**Note:** AWS 관리 콘솔에 로그인 할 땐 IAM 사용자 계정으로 진행합니다.

<br/><br/>

### 1.2. 기본 정보 확인 및 설정
Terraform 배포가 완료 후 출력되는 Output 정보에서 myeks-host의 퍼블릭 IP를 확인합니다.  
해당 IP로 EKS 관리용 인스턴스(`myeks-host`)에 SSH로 접속하고 아래 명령어를 통해 정보를 확인합니다.  

{: .box-note}
**Note:** myeks-host의 OS 변경으로 SSH 접근에 대한 계정을 ubuntu로 지정합니다.  
(ssh -i ~/.ssh/XXXX.pem ubuntu@X.X.X.X)

<br/>

<span style='color:white; background-color:#404040'> **myeks-host에 EKS 클러스터 인증 정보 업데이트** </span>

```
## EKS 클러스터 인증 정보 업데이트
aws eks update-kubeconfig --region $AWS_DEFAULT_REGION --name $CLUSTER_NAME
```

<br/>


<span style='color:white; background-color:#404040'> **kubens 설정** </span>

```
## kubectl 명령을 수행할 네임스페이스 지정
kubens default
```

<br/>

<span style='color:white; background-color:#404040'> **변수 호출 종합** </span>

```
## 배포된 EC2에 선언된 변수 호출
echo $AWS_DEFAULT_REGION
echo $CLUSTER_NAME
echo $VPCID
echo $PublicSubnet1,$PublicSubnet2,$PublicSubnet3
echo $PrivateSubnet1,$PrivateSubnet2,$PrivateSubnet3
```

{: .box-note}
**Note:** 변수 호출이 제대로 이루어지지 않을 경우 SSH 재 접속 후 다시 확인 합니다.

<br/>

<span style='color:white; background-color:#404040'> **EKS Cluster 정보 확인** </span>  
```
## eksctl을 통한 eks cluster 정보 확인
eksctl get cluster
```

<br/>

<span style='color:white; background-color:#404040'> **EKS 노드 그룹 확인** </span>  
```
## eksctl을 통한 노드 그룹 정보 확인
eksctl get nodegroup --cluster $CLUSTER_NAME --name ${CLUSTER_NAME}-node-group
  
  
## kubectl을 통한 노드 정보 확인
kubectl get node
kubectl get node -owide
```

<br/>

<span style='color:white; background-color:#404040'> **노드 IP 변수 선언 및 SSH 접근** </span>  
```
## 노드 IP 변수 저장
PublicN1=$(kubectl get node --label-columns=topology.kubernetes.io/zone --selector=topology.kubernetes.io/zone=ap-northeast-2a -o jsonpath={.items[0].status.addresses[0].address})
PublicN2=$(kubectl get node --label-columns=topology.kubernetes.io/zone --selector=topology.kubernetes.io/zone=ap-northeast-2b -o jsonpath={.items[0].status.addresses[0].address})
PublicN3=$(kubectl get node --label-columns=topology.kubernetes.io/zone --selector=topology.kubernetes.io/zone=ap-northeast-2c -o jsonpath={.items[0].status.addresses[0].address})
echo "export PublicN1=$PublicN1" >> /etc/profile
echo "export PublicN2=$PublicN2" >> /etc/profile
echo "export PublicN3=$PublicN3" >> /etc/profile
echo $PublicN1, $PublicN2, $PublicN3
  
  
## 노드에 ssh 접근 확인
for node in $PublicN1 $PublicN2 $PublicN3; do ssh -i ~/.ssh/kp_node.pem -o StrictHostKeyChecking=no ec2-user@$node hostname; done
```

<br/>

---

<br/>

## 2. Observability Backends 설치

<br/>

LGTM Observability Full Stack 환경 구성을 위한 Backend System을 설치합니다.  
[Metrics] - Grafana Mimir, [Logs] - Grafana Loki, [Traces] - Grafana Tempo

<br/>

<span style='color:white; background-color:#404040'> **사전 준비** </span>  
```
## 변수 선언
export CERT_ARN=`aws acm list-certificates --query 'CertificateSummaryList[].CertificateArn[]' --output text`
echo "export CERT_ARN=$CERT_ARN" >> /etc/profile; echo $CERT_ARN
  
export NICKNAME=[각자의 닉네임]
echo "export NICKNAME=$NICKNAME" >> /etc/profile; echo $NICKNAME
  
export OIDC_ARN=$(aws iam list-open-id-connect-providers --query 'OpenIDConnectProviderList[*].Arn' --output text)
echo "export OIDC_ARN=$OIDC_ARN" >> /etc/profile; echo $OIDC_ARN
  
export OIDC_URL=${OIDC_ARN#*oidc-provider/}
echo "export OIDC_URL=$OIDC_URL" >> /etc/profile; echo $OIDC_URL
```
  
```
## gp3 storage class 생성
cat <<EOT | kubectl apply -f -
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: gp3
allowVolumeExpansion: true
provisioner: ebs.csi.aws.com
volumeBindingMode: WaitForFirstConsumer
parameters:
  type: gp3
  allowAutoIOPSPerGBIncrease: 'true'
  encrypted: 'true'
EOT
```
  
```
## helm repo - grafana
helm repo add grafana https://grafana.github.io/helm-charts
helm repo update
```
  
```
## namespace 생성
kubectl create ns monitoring
kubectl create ns logging
kubectl create ns tracing
kubectl create ns grafana
```

<br/>

<span style='color:white; background-color:#404040'> **Values 파일 다운로드** </span>

```
## values 파일 다운로드
wget https://github.com/cloudneta/cnaeelab/raw/master/_data/values.zip
unzip values.zip; rm values.zip
  
cd values
tree
```

<br/>



### 2.1. Grafana Mimir 설치 및 확인

<br/>

<span style='color:white; background-color:#404040'> **Grafana Mimir 모니터링** </span>

```
## [모니터링1] monitoring 네임스페이스 - pod, pv, pvc, configmap 모니터링
watch kubectl get pod,pv,pvc,cm -n monitoring
```
  
```
## [모니터링2] 동적 프로비저닝으로 생성되는 EBS 볼륨 확인
while true; do aws ec2 describe-volumes \
  --filters Name=tag:ebs.csi.aws.com/cluster,Values=true \
  --query "Volumes[].{VolumeId: VolumeId, VolumeType: VolumeType, InstanceId: Attachments[0].InstanceId, State: Attachments[0].State}" \
  --output text; date; sleep 1; done
```

<br/>

<span style='color:white; background-color:#404040'> **Mimir용 S3 Bucket 생성** </span>

```
## irsa 설정 파일 경로 생성 및 진입
mkdir ~/irsa; cd ~/irsa
```
  
```
## mimir용 s3 bucket 생성 및 확인
aws s3api create-bucket \
  --bucket mimir-${NICKNAME} \
  --region $AWS_DEFAULT_REGION \
  --create-bucket-configuration LocationConstraint=$AWS_DEFAULT_REGION
  
aws s3 ls
```
  
```
## s3 bucket 이름 변수 저장
export MIMIR_BUCKET_NAME="mimir-${NICKNAME}"
echo "export MIMIR_BUCKET_NAME=$MIMIR_BUCKET_NAME" >> /etc/profile; echo $MIMIR_BUCKET_NAME
```

<br/>

<span style='color:white; background-color:#404040'> **Mimir - S3 IAM Policy 생성** </span>

```
## grafana-mimir-s3-poilcy.json 파일 생성
cat >grafana-mimir-s3-policy.json <<EOF
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "MimirStorage",
            "Effect": "Allow",
            "Action": [
                "s3:ListBucket",
                "s3:PutObject",
                "s3:GetObject",
                "s3:DeleteObject"
            ],
            "Resource": [
                "arn:aws:s3:::${MIMIR_BUCKET_NAME}",
                "arn:aws:s3:::${MIMIR_BUCKET_NAME}/*"
            ]
        }
    ]
}
EOF
cat grafana-mimir-s3-policy.json
```
  
```
## [aws-mimir-s3 IAM Policy 생성
aws iam create-policy --policy-name aws-mimir-s3 --policy-document file://grafana-mimir-s3-policy.json
```

<br/>

<span style='color:white; background-color:#404040'> **Mimir - S3 IAM Role 생성** </span>

```
## Mimir IAM Role Trust rs 생성
cat >trust-rs-mimir.json <<EOF
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
                "Federated": "${OIDC_ARN}"
            },
            "Action": "sts:AssumeRoleWithWebIdentity",
            "Condition": {
                "StringEquals": {
                    "${OIDC_URL}:sub": "system:serviceaccount:monitoring:mimir",
                    "${OIDC_URL}:aud": "sts.amazonaws.com"
                }
            }
        }
    ]
}
EOF
cat trust-rs-mimir.json
```
  
```
## AWS-Mimir-Role 생성
aws iam create-role --role-name AWS-Mimir-Role --assume-role-policy-document file://trust-rs-mimir.json
  
  
## IAM Policy와 IAM Role 연결
aws iam attach-role-policy --role-name AWS-Mimir-Role --policy-arn arn:aws:iam::${ACCOUNT_ID}:policy/aws-mimir-s3
  
  
## Mimir IAM Role ARN 변수 선언
export MIMIR_ROLE_ARN=arn:aws:iam::${ACCOUNT_ID}:role/AWS-Mimir-Role
echo "export MIMIR_ROLE_ARN=$MIMIR_ROLE_ARN" >> /etc/profile; echo $MIMIR_ROLE_ARN
```

<br/>

<span style='color:white; background-color:#404040'> **Mimir 설치** </span>

```
## MIMIR 변수 출력
export | grep MIMIR
  
  
## mimir-values.yaml 확인
cd ~/values; envsubst < mimir-temp-values.yaml > mimir-values.yaml
cat mimir-values.yaml | yh
  
  
## mimir-values 파일을 활용해서 Mimir를 helm chart로 설치
helm install mimir grafana/mimir-distributed -n monitoring -f mimir-values.yaml --version 5.4.0
```

<br/><br/>

### 2.2. Grafana Loki 설치 및 확인

<br/>

<span style='color:white; background-color:#404040'> **Grafana Loki 모니터링** </span>

```
## [모니터링1] logging 네임스페이스 - pod, pv, pvc 모니터링
watch kubectl get pod,pv,pvc -n logging
```

<br/>

<span style='color:white; background-color:#404040'> **Loki용 S3 Bucket 생성** </span>

```
## irsa 경로 이동
cd ~/irsa
```
  
```
## loki용 s3 bucket 생성
aws s3api create-bucket \
  --bucket loki-${NICKNAME} \
  --region $AWS_DEFAULT_REGION \
  --create-bucket-configuration LocationConstraint=$AWS_DEFAULT_REGION
  
aws s3 ls
```
  
```
## s3 bucket 이름 변수 저장
export LOKI_BUCKET_NAME="loki-${NICKNAME}"
echo "export LOKI_BUCKET_NAME=$LOKI_BUCKET_NAME" >> /etc/profile; echo $LOKI_BUCKET_NAME
```

<br/>

<span style='color:white; background-color:#404040'> **Loki - S3 IAM Policy 생성** </span>

```
## grafana-loki-s3-poilcy.json 파일 생성
cat >grafana-loki-s3-policy.json <<EOF
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "LokiStorage",
            "Effect": "Allow",
            "Action": [
                "s3:ListBucket",
                "s3:PutObject",
                "s3:GetObject",
                "s3:DeleteObject"
            ],
            "Resource": [
                "arn:aws:s3:::${LOKI_BUCKET_NAME}",
                "arn:aws:s3:::${LOKI_BUCKET_NAME}/*"
            ]
        }
    ]
}
EOF
cat grafana-loki-s3-policy.json
```
  
```
## aws-loki-s3 IAM Policy 생성
aws iam create-policy --policy-name aws-loki-s3 --policy-document file://grafana-loki-s3-policy.json
```

<br/>

<span style='color:white; background-color:#404040'> **Loki - S3 IAM Role 생성** </span>

```
## Loki IAM Role Trust rs 생성
cat >trust-rs-loki.json <<EOF
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
                "Federated": "${OIDC_ARN}"
            },
            "Action": "sts:AssumeRoleWithWebIdentity",
            "Condition": {
                "StringEquals": {
                    "${OIDC_URL}:sub": "system:serviceaccount:logging:loki",
                    "${OIDC_URL}:aud": "sts.amazonaws.com"
                }
            }
        },
        {
            "Effect": "Allow",
            "Principal": {
                "Federated": "${OIDC_ARN}"
            },
            "Action": "sts:AssumeRoleWithWebIdentity",
            "Condition": {
                "StringEquals": {
                    "${OIDC_URL}:sub": "system:serviceaccount:logging:loki-compactor",
                    "${OIDC_URL}:aud": "sts.amazonaws.com"
                }
            }
        }          
    ]
}
EOF
cat trust-rs-loki.json
```
  
```
## AWS-Loki-Role 생성
aws iam create-role --role-name AWS-Loki-Role --assume-role-policy-document file://trust-rs-loki.json
  
  
## IAM Policy와 IAM Role 연결
aws iam attach-role-policy --role-name AWS-Loki-Role --policy-arn arn:aws:iam::${ACCOUNT_ID}:policy/aws-loki-s3
  
  
## Loki IAM Role ARN 변수 선언
export LOKI_ROLE_ARN=arn:aws:iam::${ACCOUNT_ID}:role/AWS-Loki-Role
echo "export LOKI_ROLE_ARN=$LOKI_ROLE_ARN" >> /etc/profile; echo $LOKI_ROLE_ARN
```

<br/>

<span style='color:white; background-color:#404040'> **Loki 설치** </span>

```
## LOKI 변수 출력
export | grep LOKI
  
  
## loki-values.yaml 파일 확인
cd ~/values; envsubst < loki-temp-values.yaml > loki-values.yaml
cat loki-values.yaml | yh
  
  
## loki-values 파일을 활용해서 loki를 helm chart로 설치
helm install loki grafana/loki-distributed -n logging -f loki-values.yaml --version 0.79.1
```


<br/><br/>


### 2.3. Grafana Tempo 설치 및 확인

<br/>

<span style='color:white; background-color:#404040'> **Grafana Tempo 모니터링** </span>

```
## [모니터링1] tracing 네임스페이스 - pod, pv, pvc 모니터링
watch kubectl get pod,pv,pvc -n tracing
```

<br/>

<span style='color:white; background-color:#404040'> **Tempo용 S3 Bucket 생성** </span>

```
## irsa 경로 이동
cd ~/irsa
```
  
```
## tempo용 s3 bucket 생성
aws s3api create-bucket \
  --bucket tempo-${NICKNAME} \
  --region $AWS_DEFAULT_REGION \
  --create-bucket-configuration LocationConstraint=$AWS_DEFAULT_REGION
  
aws s3 ls
```
  
```
## s3 bucket 이름 변수 저장
export TEMPO_BUCKET_NAME="tempo-${NICKNAME}"
echo "export TEMPO_BUCKET_NAME=$TEMPO_BUCKET_NAME" >> /etc/profile; echo $TEMPO_BUCKET_NAME
```

<br/>

<span style='color:white; background-color:#404040'> **Tempo - S3 IAM Policy 생성** </span>

```
## grafana-tempo-s3-poilcy.json 파일 생성
cat >grafana-tempo-s3-policy.json <<EOF
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "TempoStorage",
            "Effect": "Allow",
            "Action": [
                "s3:ListBucket",
                "s3:PutObject",
                "s3:GetObject",
                "s3:DeleteObject"
            ],
            "Resource": [
                "arn:aws:s3:::${TEMPO_BUCKET_NAME}",
                "arn:aws:s3:::${TEMPO_BUCKET_NAME}/*"
            ]
        }
    ]
}
EOF
cat grafana-tempo-s3-policy.json
```
  
```
## aws-tempo-s3 IAM Policy 생성
aws iam create-policy --policy-name aws-tempo-s3 --policy-document file://grafana-tempo-s3-policy.json
```

<br/>

<span style='color:white; background-color:#404040'> **Tempo - S3 IAM Role 생성** </span>

```
## Tempo IAM Role Trust rs 생성
cat >trust-rs-tempo.json <<EOF
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
                "Federated": "${OIDC_ARN}"
            },
            "Action": "sts:AssumeRoleWithWebIdentity",
            "Condition": {
                "StringEquals": {
                    "${OIDC_URL}:sub": "system:serviceaccount:tracing:tempo",
                    "${OIDC_URL}:aud": "sts.amazonaws.com"
                }
            }
        }
    ]
}
EOF
cat trust-rs-tempo.json
```
  
```
## AWS-Tempo-Role 생성
aws iam create-role --role-name AWS-Tempo-Role --assume-role-policy-document file://trust-rs-tempo.json
  
  
## IAM Policy와 IAM Role 연결
aws iam attach-role-policy --role-name AWS-Tempo-Role --policy-arn arn:aws:iam::${ACCOUNT_ID}:policy/aws-tempo-s3
  
  
## Tempo IAM Role ARN 변수 선언
export TEMPO_ROLE_ARN=arn:aws:iam::${ACCOUNT_ID}:role/AWS-Tempo-Role
echo "export TEMPO_ROLE_ARN=$TEMPO_ROLE_ARN" >> /etc/profile; echo $TEMPO_ROLE_ARN
```

<br/>

<span style='color:white; background-color:#404040'> **IAM Policy와 Role 확인** </span>

```
## Mimir, Loki, Tempo - S3 IAM Policy 확인
aws iam list-policies \
  --query 'Policies[?contains(PolicyName, `mimir`) || contains(PolicyName, `loki`) || contains(PolicyName, `tempo`)].PolicyName' \
  --output text
```
  
```
## Mimir, Loki, Tempo - S3 IAM Role 확인
aws iam list-roles \
  --query 'Roles[?contains(RoleName, `Mimir`) || contains(RoleName, `Loki`) || contains(RoleName, `Tempo`)].RoleName' \
  --output text
```


<br/>

<span style='color:white; background-color:#404040'> **Tempo 설치** </span>

```
## TEMPO 변수 출력
export | grep TEMPO
  
  
## tempo-values.yaml 파일 확인
cd ~/values; envsubst < tempo-temp-values.yaml > tempo-values.yaml
cat tempo-values.yaml | yh
  
  
## tempo-values 파일을 활용해서 tempo를 helm chart로 설치
helm install tempo grafana/tempo-distributed -n tracing -f tempo-values.yaml --version 1.15.2
```

<br/>

<span style='color:white; background-color:#404040'> **Observability Backend 설치 확인** </span>

```
## helm chart로 설치한 Obervability Backend 시스템 확인
for ns in monitoring logging tracing; do helm list -n $ns; done
```


<br/><br/>


---

<br/>

## 3. OpenTelemetry Operator & Collector 설치

<br/>

OpenTelemetry Operator를 통해 OpenTelemetry CRD를 생성해서 OTel Collector, OTel Auto-Instrumentation을 설치할 수 있습니다.

<br/>

### 3.1. OpenTelemetry Operator 설치

<br/>

<span style='color:white; background-color:#404040'> **cert-manager 설치 및 확인** </span>

```
## helm repository 추가
helm repo add jetstack https://charts.jetstack.io --force-update
helm repo update
```
  
```
## cert-manager 설치
helm install \
  cert-manager jetstack/cert-manager \
  --namespace cert-manager \
  --create-namespace \
  --version v1.15.1 \
  --set crds.enabled=true
```
  
```
## cert-manager 생성 확인
kubectl get pod -n cert-manager
  
  
## cert-manager crd 확인
kubectl get crd | grep cert-manager
```

<br/>

<span style='color:white; background-color:#404040'> **OpenTelemetry Operator 설치 및 확인** </span>

```
## 모니터링 - otel 네임스페이스
watch kubectl get all -n otel
```
  
```
## helm repository 추가
helm repo add open-telemetry https://open-telemetry.github.io/opentelemetry-helm-charts
helm repo update
```
  
```
## otel-operator 설치
helm install opentelemetry-operator open-telemetry/opentelemetry-operator \
  --namespace otel \
  --create-namespace \
  --set "manager.collectorImage.repository=otel/opentelemetry-collector-k8s"
```
  
```
## otel-operator crd 설치 확인
kubectl get crd | grep open
```

<br/>

### 3.2. OpenTelemetry Collector 생성

<br/>

<span style='color:white; background-color:#404040'> **OpenTelemetry Collector 생성** </span>

```
## otelcol-values.yaml 확인
cd ~/values; cat otelcol-values.yaml | yh
  
  
## Otel Collector 설치
helm install opentelemetry open-telemetry/opentelemetry-collector -n otel -f otelcol-values.yaml --version 0.81.0
```

<br/><br/>

---

<br/>

## 4. Observability Visualization 설치

<br/>

Observability Visualization 도구로 Grafana를 설치하고 결과를 확인합니다.

<br/>

### 4.1. Grafana 설치

<br/>

<span style='color:white; background-color:#404040'> **Grafana 모니터링** </span>

```
## [모니터링1] grafana 네임스페이스 - pod, svc, ep, ingress, pv, pvc 모니터링
watch kubectl get pod,svc,ep,ingress,pv,pvc -n grafana
```
  
```
## [모니터링2] 동적 프로비저닝으로 생성되는 EBS 볼륨 확인
while true; do aws ec2 describe-volumes \
  --filters Name=tag:ebs.csi.aws.com/cluster,Values=true \
  --query "Volumes[].{VolumeId: VolumeId, VolumeType: VolumeType, InstanceId: Attachments[0].InstanceId, State: Attachments[0].State}" \
  --output text; date; sleep 1; done
```

<br/>

<span style='color:white; background-color:#404040'> **Grafana 설치** </span>

```
## MyDomain 변수 선언
export MyDomain=[각자 도메인 네임]
echo "export MyDomain=$MyDomain" >> /etc/profile; echo $MyDomain
```
  
```
## grafana-values.yaml 확인
cd ~/values; envsubst < grafana-temp-values.yaml > grafana-values.yaml
cat grafana-values.yaml | yh
  
  
## grafana-values 파일을 활용해서 grafana를 helm chart로 설치
helm install grafana grafana/grafana -n grafana -f grafana-values.yaml
```

<br/>

{: .box-note}
**Note:** https://도메인주소/grafana로 접속합니다. (id: admin, pw: qwer1234)

<br/><br/>

---

<br/>

## 5. LGTM Observability 검증

<br/>

### 5.1. [Metrics] Grafana Mimir 확인

<br/>

<span style='color:white; background-color:#404040'> **Grafana Data Source 설정** </span>

Grafana - Data Source 지정 [Data Sources -> Add data source -> Prometheus]  
- Name: Mimir  
- Connection - Prometheus server URL: `http://mimir-nginx.monitoring/prometheus`  
- Save & Test

<br/>

<details>
  <summary><span style='color:gray'>이미지 빌드</span></summary>
  <div markdown="1">
  <br/>
  <span style='color:white; background-color:#404040'> **이미지 빌드** </span>
    <br/>
    <code>
    ## python 도구 설치
    apt install python3-pip -y
    pip install flask prometheus-flask-exporter
    </code>
  </div>
</details>


<details>
  <summary><span style='color:gray'>이미지 빌드</span></summary>
  <div markdown="1">
    <pre><code>
    // 실습 디렉터리 경로 진입
    cd cnaee_class_tf/ch3
    </code></pre>
  </div>
</details>


<span style='color:white; background-color:#404040'> **Test Pod 생성** </span>

```
## test-pod.yaml 생성
cd; cat >test-pod.yaml <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: python-metric-test
  labels:
    app: python-metric
  annotations:
    prometheus.io/scrape: 'true'
    prometheus.io/port: '5000'
spec:
  containers:
  - name: python-app
    image: ongja/my-flask-app:latest
    ports:
    - containerPort: 5000
EOF
```
  
```
## test pod 배포
kubectl apply -f test-pod.yaml
  
  
## test pod에 cpu 부하
kubectl exec -it python-metric-test -- /bin/sh -c "apt-get update && apt-get install -y stress && stress --cpu 4 --timeout 300"
```

<br/>

<span style='color:white; background-color:#404040'> **Grafana에서 Mimir 정보 확인** </span>

Grafana 웹 페이지에서 Explorer 메뉴로 진입합니다.  
- Outline: Mimir  
- Builder -> Code  
- Last 5 minutes  
- Run Query  

```
## 대상 파드의 누적 CPU 사용
container_cpu_usage_seconds_total{pod="python-metric-test"}
```
  
```
## 대상 파드의 CPU 사용량이 전체 CPU에 차지하는 비율
(sum(rate(container_cpu_usage_seconds_total{namespace="default", pod="python-metric-test"}[5m])) / sum(rate(container_cpu_usage_seconds_total[5m]))) * 100
```

<br/>

### 5.2. [Logs] Grafana Loki 확인

<br/>

<span style='color:white; background-color:#404040'> **Grafana Data Source 설정** </span>

Grafana - Data Source 지정 [Data Sources -> Add data source -> Loki]  
- Name: Loki  
- Connection - Loki server URL: `http://loki-loki-distributed-gateway.logging`  
- Save & Test

<br/>

<span style='color:white; background-color:#404040'> **Grafana에서 Loki 정보 확인** </span>

Grafana 웹 페이지에서 Explorer 메뉴로 진입합니다.  
- Outline: Loki  
- Builder -> Code  
- Last 5 minutes  
- Run Query  

```
## 대상 파드의 모든 로그 정보 확인
{k8s_pod_name="python-metric-test"}
```

<br/>

<span style='color:white; background-color:#404040'> **파드 로그 정보 확인** </span>
 
```
## otel collector 설정 정보 필터링
cat ~/values/otelcol-values.yaml | grep -A 1 "filelog:"
```
  
```
## 노드에 구성된 파드의 로그 확인
for node in $PublicN1 $PublicN2 $PublicN3; do 
    ssh -i ~/.ssh/kp_node.pem ec2-user@$node "sudo sh -c 'ls /var/log/pods/default*/python-app/'" 
done
  
for node in $PublicN1 $PublicN2 $PublicN3; do 
    ssh -i ~/.ssh/kp_node.pem ec2-user@$node "sudo sh -c 'cat /var/log/pods/default*/python-app/0.log'" 
done
```

<br/>



### 5.3. [Traces] Grafana Tempo 확인

<br/>

<span style='color:white; background-color:#404040'> **Grafana Data Source 설정** </span>

Grafana - Data Source 지정 [Data Sources -> Add data source -> Tempo]  
- Name: Tempo  
- Connection - Tempo server URL: `http://tempo-query-frontend-discovery.tracing:3100`  
- Save & Test

<br/>

<details>
<summary><span style='color:gray'>이미지 빌드</span></summary>
<div markdown="1">

<br/>

<span style='color:white; background-color:#404040'> **이미지 빌드** </span>

```
## python 도구 설치
apt install python3-pip
pip install flask prometheus-flask-exporter
  
  
## 이미지 빌드를 위한 디렉터리 생성 및 이동
mkdir ~/buildimage2 && cd ~/buildimage2
  
  
## 1.py 작성
cat << EOF > 1.py
from flask import Flask, request
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.resources import Resource
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
from opentelemetry.semconv.resource import ResourceAttributes

app = Flask(__name__)

resource = Resource.create({
    ResourceAttributes.SERVICE_NAME: "MyAwesomeService",
    ResourceAttributes.SERVICE_NAMESPACE: "Production",
    ResourceAttributes.SERVICE_VERSION: "1.0.0"
})

trace.set_tracer_provider(TracerProvider(resource=resource))
tracer = trace.get_tracer(__name__)

otlp_exporter = OTLPSpanExporter(endpoint="http://tempo-distributor-discovery.tracing:4317", insecure=True)
trace.get_tracer_provider().add_span_processor(BatchSpanProcessor(otlp_exporter))

@app.route("/")
def hello_world():
    with tracer.start_as_current_span("hello-world-operation"):
        return "Hello, World!"

if __name__ == "__main__":
    app.run(debug=True, host='0.0.0.0')
EOF
  
  
## requirements.txt 생성
cat << EOF > requirements.txt
Flask
opentelemetry-api
opentelemetry-sdk
opentelemetry-exporter-otlp
werkzeug==2.0.0
EOF
  
  
## Dockerfile 작성
cat << EOF > Dockerfile
FROM python:3.8-slim
WORKDIR /app
COPY . /app

COPY requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt

CMD ["python", "1.py"]
EOF
  
  
## docker build
docker build -t inst-1 .
docker images
  
docker login
docker build --no-cache -t ongja/inst-1:latest .
docker push ongja/inst-1:latest
```

<br/>

</div>
</details>

<span style='color:white; background-color:#404040'> **Trace 수동 계측** </span>

```
## 1_deploy.yaml 생성
cat << EOF > 1_deploy.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: python-app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: python-app
  template:
    metadata:
      labels:
        app: python-app
    spec:
      containers:
      - name: python-app
        image: ongja/inst-1:latest
        ports:
        - containerPort: 5000
---
apiVersion: v1
kind: Service
metadata:
  name: python-app-service
spec:
  selector:
    app: python-app
  ports:
    - protocol: TCP
      port: 80
      targetPort: 5000
EOF
```
  
```
## 배포 및 확인
kubectl apply -f 1_deploy.yaml
  
kubectl get pod,svc
  
  
## 접근 확인
kubectl run curl --image=appropriate/curl --restart=Never --rm -it -- sh
  
curl http://python-app-service.default.svc.cluster.local
```

<br/>





<details>
<summary><span style='color:gray'>이미지 빌드</span></summary>
<div markdown="1">

<br/>

<span style='color:white; background-color:#404040'> **이미지 빌드** </span>

```
## python 도구 설치
apt install python3-pip
pip install flask prometheus-flask-exporter
  
  
## 이미지 빌드를 위한 디렉터리 생성 및 이동
mkdir ~/buildimage3 && cd ~/buildimage3
  
  
## app-1.py 작성
cat << EOF > app-1.py
from flask import Flask, jsonify, abort
import requests
import os

app = Flask(__name__)

SERVICE_B_HOST = os.environ.get('SERVICE_B_HOST', 'localhost')
SERVICE_B_PORT = os.environ.get('SERVICE_B_PORT', '5001')

@app.route('/')
def call_app_b():
    url = f'http://{SERVICE_B_HOST}:{SERVICE_B_PORT}/'
    try:
        response = requests.get(url)
        return jsonify({'message': "I'm app-1!", 'response': response.json()})
    except requests.RequestException as e:
        return jsonify({'error': str(e)}), 500

@app.route('/error')
def error():
    return abort(500)

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=int(os.environ.get('PORT', 5000)))
EOF
  
  
## app-2.py
cat << EOF > app-2.py
from flask import Flask, jsonify, abort
import requests
import os

app = Flask(__name__)

SERVICE_C_HOST = os.environ.get('SERVICE_C_HOST', 'localhost')
SERVICE_C_PORT = os.environ.get('SERVICE_C_PORT', '5002')

@app.route('/')
def call_app_c():
    url = f'http://{SERVICE_C_HOST}:{SERVICE_C_PORT}/'
    try:
        response = requests.get(url)
        return jsonify({'message': "I'm app-2!", 'response': response.json()})
    except requests.RequestException as e:
        return jsonify({'error': str(e)}), 500

@app.route('/error')
def error():
    return abort(503)

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=int(os.environ.get('PORT', 5001)))
EOF
  
  
## app-3.py
cat << EOF > app-3.py
from flask import Flask, jsonify
import os

app = Flask(__name__)

@app.route('/')
def home():
    return jsonify({'message': "I'm app-3!"})

@app.route('/error')
def error():
    return jsonify({'message': 'Error in app-3!'}), 404

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=int(os.environ.get('PORT', 5002)))
EOF
  
  
## Dockerfile-app-1
cat << EOF > Dockerfile-app-1
FROM python:3.8-slim
WORKDIR /app
COPY . /app
RUN apt-get update && \
    apt-get install -y curl && \
    rm -rf /var/lib/apt/lists/* && \
    pip install Flask requests
RUN pip install opentelemetry-distro opentelemetry-exporter-otlp opentelemetry-api opentelemetry-sdk
RUN opentelemetry-bootstrap -a install
CMD ["opentelemetry-instrument", "python", "app-1.py"]
EOF
  
  
## Dockerfile-app-2
cat << EOF > Dockerfile-app-2
FROM python:3.8-slim
WORKDIR /app
COPY . /app
RUN apt-get update && \
    apt-get install -y curl && \
    rm -rf /var/lib/apt/lists/* && \
    pip install Flask requests
RUN pip install opentelemetry-distro opentelemetry-exporter-otlp opentelemetry-api opentelemetry-sdk opentelemetry-instrumentation-flask
RUN opentelemetry-bootstrap -a install
CMD ["opentelemetry-instrument", "python", "app-2.py"]
EOF
  
  
## Dockerfile-app-3
cat << EOF > Dockerfile-app-3
FROM python:3.8-slim
WORKDIR /app
COPY . /app
RUN apt-get update && \
    apt-get install -y curl && \
    rm -rf /var/lib/apt/lists/* && \
    pip install Flask requests
RUN pip install opentelemetry-distro opentelemetry-exporter-otlp opentelemetry-api opentelemetry-sdk opentelemetry-instrumentation-flask
RUN opentelemetry-bootstrap -a install
CMD ["opentelemetry-instrument", "python", "app-3.py"]
EOF
  
  
## docker build
docker login
  
docker build -f Dockerfile-app-1 -t ongja/tempo-app-1:latest .
docker build -f Dockerfile-app-2 -t ongja/tempo-app-2:latest .
docker build -f Dockerfile-app-3 -t ongja/tempo-app-3:latest .
  
docker push ongja/tempo-app-1:latest
docker push ongja/tempo-app-2:latest
docker push ongja/tempo-app-3:latest
```

<br/>

</div>
</details>

<span style='color:white; background-color:#404040'> **Trace 프로그래밍 계측** </span>

```
## app-1.yaml 생성
cat << EOF > app-1.yaml
apiVersion: v1
kind: Service
metadata:
  name: app-1
spec:
  selector:
    app: app-1
  ports:
  - protocol: TCP
    port: 80
    targetPort: 5000
    name: http
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-1
spec:
  replicas: 1
  selector:
    matchLabels:
      app: app-1
  template:
    metadata:
      labels:
        app: app-1
    spec:
      containers:
      - name: app-1
        image: ongja/tempo-app-1:latest
        command: ["/bin/sh"]
        args:
        - "-c"
        - |
          OTEL_EXPORTER_OTLP_TRACES_ENDPOINT=http://tempo-distributor-discovery.tracing:4318/v1/traces opentelemetry-instrument python app-1.py
        ports:
        - containerPort: 5000
          name: http
        env:
        - name: SERVICE_B_HOST
          value: "app-2"
        - name: SERVICE_B_PORT
          value: "80"
        - name: PORT
          value: "5000"
        - name: OTEL_SERVICE_NAME
          value: "app-1"
        - name: OTEL_TRACES_EXPORTER
          value: "console,otlp"
        - name: OTEL_EXPORTER_OTLP_TRACES_HEADERS
          value: "api-key=key,other-config-value=value"
        - name: OTEL_TRACES_SAMPLER_ARG
          value: "1"
        - name: OTEL_EXPORTER_OTLP_PROTOCOL
          value: "http/protobuf"
        - name: OTEL_METRICS_EXPORTER
          value: "none"
EOF
```
  
```
## app-2.yaml 생성
cat << EOF > app-2.yaml
apiVersion: v1
kind: Service
metadata:
  name: app-2
spec:
  selector:
    app: app-2
  ports:
  - protocol: TCP
    port: 80
    targetPort: 5001
    name: http
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-2
spec:
  replicas: 1
  selector:
    matchLabels:
      app: app-2
  template:
    metadata:
      labels:
        app: app-2
    spec:
      containers:
      - name: app-2
        image: ongja/tempo-app-2:latest
        command: ["/bin/sh"]
        args:
        - "-c"
        - |
          OTEL_EXPORTER_OTLP_TRACES_ENDPOINT=http://tempo-distributor-discovery.tracing:4318/v1/traces opentelemetry-instrument python app-2.py
        ports:
        - containerPort: 5001
          name: http
        env:
        - name: PORT
          value: "5001"
        - name: SERVICE_C_HOST
          value: "app-3"
        - name: SERVICE_C_PORT
          value: "80"
        - name: OTEL_SERVICE_NAME
          value: "app-2"
        - name: OTEL_TRACES_SAMPLER_ARG
          value: "100"
        - name: OTEL_TRACES_EXPORTER
          value: "console,otlp"
        - name: OTEL_EXPORTER_OTLP_PROTOCOL
          value: "http/protobuf"
        - name: OTEL_METRICS_EXPORTER
          value: "none"
EOF
```
  
```
## app-3.yaml 생성
cat << EOF > app-3.yaml
apiVersion: v1
kind: Service
metadata:
  name: app-3
spec:
  selector:
    app: app-3
  ports:
  - protocol: TCP
    port: 80
    targetPort: 5002
    name: http
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-3
spec:
  replicas: 1
  selector:
    matchLabels:
      app: app-3
  template:
    metadata:
      labels:
        app: app-3
    spec:
      containers:
      - name: app-3
        image: ongja/tempo-app-3:latest
        command: ["/bin/sh"]
        args:
        - "-c"
        - |
          OTEL_EXPORTER_OTLP_TRACES_ENDPOINT=http://tempo-distributor-discovery.tracing:4318/v1/traces opentelemetry-instrument python app-3.py
        ports:
        - containerPort: 5002
          name: http
        env:
        - name: PORT
          value: "5002"
        - name: OTEL_SERVICE_NAME
          value: "app-3"
        - name: OTEL_EXPORTER_OTLP_TRACES_HEADERS
          value: "api-key=key,other-config-value=value"
        - name: OTEL_TRACES_SAMPLER_ARG
          value: "100"
        - name: OTEL_EXPORTER_OTLP_PROTOCOL
          value: "http/protobuf"
        - name: OTEL_METRICS_EXPORTER
          value: "none"
        - name: OTEL_TRACES_EXPORTER
          value: "console,otlp"
EOF
```
  
```
## 배포 및 확인
kubectl create ns tempo-test
  
kubectl apply -f app-1.yaml -n tempo-test
kubectl apply -f app-2.yaml -n tempo-test
kubectl apply -f app-3.yaml -n tempo-test
  
kubectl get pod,svc -n tempo-test
```
  
```
## 접근 확인
kubectl run -n tempo-test curl --image=appropriate/curl --restart=Never --rm -it -- sh
  
curl http://app-1
curl http://app-1/error
curl http://app-2
curl http://app-2/error
curl http://app-3
curl http://app-3/error
```

<br/>


<span style='color:white; background-color:#404040'> **Trace 자동 계측 - auto-instrumentation 생성** </span>

```
## auto-instrumentation.yaml 생성
cat >auto-instrumentation.yaml <<EOF
apiVersion: opentelemetry.io/v1alpha1
kind: Instrumentation
metadata:
  name: otel-instrumentation
spec:
  exporter:
    endpoint: http://opentelemetry-opentelemetry-collector.otel.svc.cluster.local:4317
  propagators:
  - tracecontext
  - baggage
  sampler:
    type: parentbased_traceidratio
    argument: "1"
  nodejs:
    env:
      - name: OTEL_EXPORTER_OTLP_TRACES_PROTOCOL
        value: "grpc"
      - name: OTEL_EXPORTER_OTLP_METRICS_PROTOCOL
        value: "grpc"
      - name: OTEL_NODE_DISABLED_INSTRUMENTATIONS
        value: "fs"
EOF
```
  
```
## auto-instrumentation 배포 및 확인
kubectl apply -f auto-instrumentation.yaml
  
kubectl get instrumentations.opentelemetry.io
```

<br/>


<span style='color:white; background-color:#404040'> **Trace 자동 계측 - java 환경 테스트** </span>

```
## spring-java.yaml 생성
cat >spring-java.yaml <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: java-app
spec:
  selector:
    matchLabels:
      app: java-app
  replicas: 1
  template:
    metadata:
      labels:
        app: java-app
      annotations:
        sidecar.opentelemetry.io/inject: "true"
        instrumentation.opentelemetry.io/inject-java: "true"
    spec:
      containers:
      - name: app
        image: springio/gs-spring-boot-docker
---
apiVersion: v1
kind: Service
metadata:
  name: java-app-service
spec:
  type: NodePort
  selector:
    app: java-app
  ports:
  - protocol: TCP
    port: 8080
    targetPort: 8080
EOF
```
  
```
## spring-java.yaml 배포
kubectl apply -f spring-java.yaml
  
  
## 테스트 파드에 접근해서 확인
kubectl run curl --image=appropriate/curl --restart=Never --rm -it -- sh
  
curl http://java-app-service:8080
```

<br/>

<span style='color:white; background-color:#404040'> **Trace 자동 계측 - nodejs 환경 테스트** </span>

```
## nodejs-test.yaml 생성
cat >nodejs-test.yaml <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nodejs-express
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nodejs-express
  template:
    metadata:
      labels:
        app: nodejs-express
      annotations:
        instrumentation.opentelemetry.io/inject-nodejs: "true"
    spec:
      containers:
      - name: nodejs-express
        image: bitnami/node:14
        command: ["sh", "-c"]
        args: ["npm install express && node -e \"const express = require('express'); const app = express(); app.get('/', (req, res) => res.send('Hello World!')); app.listen(8080);\""]
        ports:
        - containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: nodejs-express-service
spec:
  type: NodePort
  selector:
    app: nodejs-express
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8080
EOF
```
  
```
## nodejs-test.yaml 배포
kubectl apply -f nodejs-test.yaml
  
  
## 테스트 파드에 접근해서 확인
kubectl run curl --image=appropriate/curl --restart=Never --rm -it -- sh
  
curl http://nodejs-express-service
```

<br/><br/>


---

<br/>

## 6. 실습 환경 삭제

<br/>

3장 LGTM Observability Full Stack 실습 환경 삭제와 Terraform 삭제 작업 모두를 진행합니다.

<br/>

### 6.1. 실습 자원 삭제

<br/>

<span style='color:white; background-color:#404040'> **실습 애플리케이션 삭제** </span>

```
cd ~
  
## 실습 애플리케이션 삭제
kubectl delete -f nodejs-test.yaml
kubectl delete -f spring-java.yaml
kubectl delete -f 1_deploy.yaml
kubectl delete -f 1_service.yaml
kubectl delete -f test-pod.yaml
  
kubectl delete -f app-1.yaml -n tempo-test
kubectl delete -f app-2.yaml -n tempo-test
kubectl delete -f app-3.yaml -n tempo-test
```

<br/>

<span style='color:white; background-color:#404040'> **helm과 pvc 삭제** </span>

```
helm uninstall grafana -n grafana
  
helm uninstall opentelemetry -n otel
  
helm uninstall opentelemetry-operator -n otel
  
helm uninstall cert-manager -n cert-manager
  
helm uninstall tempo -n tracing
kubectl delete pvc --all -n tracing
  
helm uninstall loki -n logging
kubectl delete pvc --all -n logging
  
helm uninstall mimir -n monitoring
kubectl delete pvc --all -n monitoring
  
helm uninstall external-dns -n kube-system
  
helm uninstall aws-load-balancer-controller -n kube-system
```

<br/>

<span style='color:white; background-color:#404040'> **Observability Backend - S3 IAM Policy와 Role 삭제** </span>

```
aws iam detach-role-policy --role-name AWS-Loki-Role --policy-arn arn:aws:iam::${ACCOUNT_ID}:policy/aws-loki-s3
aws iam detach-role-policy --role-name AWS-Mimir-Role --policy-arn arn:aws:iam::${ACCOUNT_ID}:policy/aws-mimir-s3
aws iam detach-role-policy --role-name AWS-Tempo-Role --policy-arn arn:aws:iam::${ACCOUNT_ID}:policy/aws-tempo-s3
  
aws iam delete-policy --policy-arn arn:aws:iam::${ACCOUNT_ID}:policy/aws-loki-s3
aws iam delete-policy --policy-arn arn:aws:iam::${ACCOUNT_ID}:policy/aws-mimir-s3
aws iam delete-policy --policy-arn arn:aws:iam::${ACCOUNT_ID}:policy/aws-tempo-s3
  
aws iam delete-role --role-name AWS-Loki-Role
aws iam delete-role --role-name AWS-Mimir-Role
aws iam delete-role --role-name AWS-Tempo-Role
```

<br/>

<span style='color:white; background-color:#404040'> **S3 Bucket 삭제** </span>

```
aws s3 rm s3://$MIMIR_BUCKET_NAME --recursive
aws s3 rm s3://$LOKI_BUCKET_NAME --recursive
aws s3 rm s3://$TEMPO_BUCKET_NAME --recursive
  
aws s3api delete-bucket --bucket $MIMIR_BUCKET_NAME --region $AWS_DEFAULT_REGION
aws s3api delete-bucket --bucket $LOKI_BUCKET_NAME --region $AWS_DEFAULT_REGION
aws s3api delete-bucket --bucket $TEMPO_BUCKET_NAME --region $AWS_DEFAULT_REGION
  
aws s3 ls
```

<br/>


### 6.2. Terraform 삭제

<br/>

<span style='color:white; background-color:#404040'> **Terraform 삭제** </span>

```
## terraform 자원 삭제
terraform destroy -auto-approve
```

<br/>


{: .box-warning}
**Warning:** Terraform 삭제 동안 터미널을 유지하고 Terraform 삭제가 완료되면 정상적으로 자원 삭제 되었는지 꼭 확인을 합니다.

<br/>

---

<br/>

여기까지 3장의 첫 번째 실습인 LGTM Observability Full Stack 실습을 마칩니다.  
수고하셨습니다 :)

<br/><br/>
