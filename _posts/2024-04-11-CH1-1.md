---
layout: post
title: 1장 실습 Amazon EKS Fully Private Cluster 구성하기
subtitle: Amazon EKS의 완전 프라이빗 클러스터를 구성하고 동작을 확인합니다.
tags: [eks, 1장]
published: true
---
|목차|
|-----------|
|[1. 기본 환경 배포](#1-기본-환경-배포)|
|[&nbsp;&nbsp;&nbsp;&nbsp;1.1. Terraform을 통한 기본 인프라 배포](#11-terraform을-통한-기본-인프라-배포)|
|[&nbsp;&nbsp;&nbsp;&nbsp;1.2. 기본 정보 확인 및 설정](#12-기본-정보-확인-및-설정)|
|[&nbsp;&nbsp;&nbsp;&nbsp;1.3. API Server - Endpoint Public Access 통신 흐름](#13-api-server---endpoint-public-access-통신-흐름)|
|[2. EKS Fully Private Cluster 배포 및 확인](#2-eks-fully-private-cluster-배포-및-확인)|
|[&nbsp;&nbsp;&nbsp;&nbsp;2.1. EKS Fully Private Cluster 배포](#21-eks-fully-private-cluster-배포)|
|[&nbsp;&nbsp;&nbsp;&nbsp;2.2. EKS Fully Private Cluster 정보 확인 및 설정](#22-eks-fully-private-cluster-정보-확인-및-설정)|
|[&nbsp;&nbsp;&nbsp;&nbsp;2.3. API Server - Endpoint Private Access 통신 흐름](#23-api-server---endpoint-private-access-통신-흐름)|
|[&nbsp;&nbsp;&nbsp;&nbsp;2.4. 파드 구성 및 통신 확인](#24-파드-구성-및-통신-확인)|
|[3. 실습 환경 삭제](#3-실습-환경-삭제)|
|[&nbsp;&nbsp;&nbsp;&nbsp;3.1. EKS Fully Private Cluster 삭제](#31-eks-fully-private-cluster-삭제)|
|[&nbsp;&nbsp;&nbsp;&nbsp;3.2. Terraform 삭제 또는 유지](#32-terraform-삭제-또는-유지)|

<br/>


## 1. 기본 환경 배포

<br/>

이번 실습은 IAM 사용자 계정을 통해 관리 콘솔에 접근하고 액세스 키를 활용해 awscli 도구를 사용합니다.  
해당 작업을 수행하지 않았다면 아래 토글을 확장해 작업을 선행하고 본격적인 실습에 들어갑니다.

<details>
<summary><span style='color:orange'>IAM 사용자 생성 및 액세스 키 생성</span></summary>
<div markdown="1">

<br/>

<span style='color:white; background-color:#404040'> **IAM 사용자 생성** </span>

- 루트 계정으로 로그인하여 [링크](https://us-east-1.console.aws.amazon.com/iamv2/home#/users){:target="_blank"}에 클릭하여 IAM 사용자 페이지에 진입합니다.
- `사용자 추가` 버튼을 클릭합니다.
- <U>사용자 이름</U>은 *admin*으로 입력하고 [AWS Management Console에 대한 사용자 액세스 권한 제공]을 체크합니다.
- <U>사용자에게 콘솔 액세스 권한 제공</U>은 [IAM 사용자를 생성하고 싶음]을 선택합니다.
- <U>콘솔 암호</U>는 [사용자 지정 암호]를 선택하고 생성 기준에 맞춰 각자 암호를 지정합니다.
- <U>사용자는 다음 로그인 시 새 암호를 생성해야 합니다.</U>를 체크 해제하고 `다음` 버튼을 클릭합니다.
- <U>권한 옵션</U>은 [직접 정책 연결]을 선택하고 <U>권한 정책</U>에서 [AdministratorAccess]를 체크한 후 아래 `다음` 버튼을 클릭합니다.
- 검토 및 생성 페이지에서 `사용자 생성` 버튼을 클릭합니다.
- 암호 검색 페이지에서 `.csv 파일 다운로드` 버튼을 클릭하여 자신의 PC의 디렉터리에 저장합니다.
- `사용자 목록으로 돌아가기` 버튼을 클릭하여 IAM 사용자 생성을 마무리합니다.

<br/>

<span style='color:white; background-color:#404040'> **IAM 사용자 액세스 키 생성** </span>

- IAM 사용자 페이지에서 `생성한 사용자 이름`을 클릭합니다.
- `보안 자격 증명` 탭을 클릭하고 [액세스 키] 영역에서 `액세스 키 만들기` 버튼을 클릭합니다.
- 액세스 키 모범 사례 및 대안 페이지에서 [Command Line Interface(CLI)]를 선택하고 아래 체크 박스를 체크한 후 `다음` 버튼을 클릭합니다.
- `액세스 키 만들기` 버튼을 클릭합니다.
- 액세스 키 검색 페이지에서 `.csv 파일 다운로드` 버튼을 클릭하여 자신의 PC의 디렉터리에 저장합니다.
- `완료` 버튼을 클릭하여 IAM 사용자 액세스 키 생성을 마무리합니다.

{: .box-note}
**Note:** IAM 사용자로 관리 콘솔에 로그인 할때 계정 ID가 필요하니 잘 메모해 둡니다.

</div>
</details>


<br/>

### 1.1. Terraform을 통한 기본 인프라 배포

Terraform을 통한 기본 인프라 배포에 앞서 SSH 키 페어, IAM User Access Key ID, IAM User Secret Access Key, 작업 PC의 공인 IP 주소를 미리 확인하고 메모해 둡니다.

<br/>

<span style='color:white; background-color:#404040'> **Terraform으로 기본 인프라 배포** </span>

{% highlight javascript linenos %}
// 실습 디렉터리 경로 진입
cd cnaee_class_tf/ch1

// Terraform 환경 변수 저장
export TF_VAR_KeyName=[각자 ssh keypair]
export TF_VAR_MyIamUserAccessKeyID=[각자 iam 사용자의 access key id]
export TF_VAR_MyIamUserSecretAccessKey=[각자 iam 사용자의 secret access key]
export TF_VAR_SgIngressSshCidr=$(curl -s ipinfo.io/ip)/32

//Terraform 배포
terraform init

terraform plan

terraform apply -auto-approve
{% endhighlight %}

<br/>

Terraform을 통한 기본 인프라 배포가 완료되면 관리 콘솔에서 생성된 인프라들을 확인합니다.

{: .box-note}
**Note:** AWS 관리 콘솔에 로그인 할 땐 IAM 사용자 계정으로 진행합니다.

<br/><br/>

### 1.2. 기본 정보 확인 및 설정
Terraform 배포가 완료 후 출력되는 Output 정보에서 myeks-host의 퍼블릭 IP를 확인합니다.  
해당 IP로 EKS 관리용 인스턴스(`myeks-host`)에 SSH로 접속하고 아래 명령어를 통해 정보를 확인합니다.

<br/>

<span style='color:white; background-color:#404040'> **myeks-host에 EKS 클러스터 인증 정보 업데이트** </span>

{% highlight javascript linenos %}
// EKS 클러스터 인증 정보 업데이트
aws eks update-kubeconfig --region $AWS_DEFAULT_REGION --name $CLUSTER_NAME
{% endhighlight %}

<br/>


<span style='color:white; background-color:#404040'> **kubens 설정** </span>

{% highlight javascript linenos %}
// kubectl 명령을 수행할 네임스페이스 지정
kubens default
{% endhighlight %}

<br/>

<span style='color:white; background-color:#404040'> **변수 호출 종합** </span>

{% highlight javascript linenos %}
// 배포된 EC2에 선언된 변수 호출
echo $AWS_DEFAULT_REGION
echo $CLUSTER_NAME
echo $VPCID
echo $PublicSubnet1,$PublicSubnet2,$PublicSubnet3
echo $PrivateSubnet1,$PrivateSubnet2,$PrivateSubnet3
{% endhighlight %}

{: .box-note}
**Note:** 변수 호출이 제대로 이루어지지 않을 경우 SSH 재 접속 후 다시 확인 합니다.

<br/>

<span style='color:white; background-color:#404040'> **EKS Cluster 정보 확인** </span>  
{% highlight javascript linenos %}
// kubectl을 통한 eks cluster 정보 확인
kubectl cluster-info

// eksctl을 통한 eks cluster 정보 확인
eksctl get cluster

// awscli를 통한 eks cluster 정보 확인 (상세)
aws eks describe-cluster --name $CLUSTER_NAME | jq

// awscli를 통한 eks cluster 정보 확인 (endpoint 주소와 endpoint access 확인)
aws eks describe-cluster --name $CLUSTER_NAME | grep endpoint
{% endhighlight %}

<br/>

<span style='color:white; background-color:#404040'> **API Server Endpoint 주소 확인 및 조회** </span>  
{% highlight javascript linenos %}
// API Server Endpoint 도메인 변수 선언
APIDNS_PUBLIC=$(aws eks describe-cluster --name ${CLUSTER_NAME} | jq -r .cluster.endpoint | cut -d '/' -f 3)
echo $APIDNS_PUBLIC
echo "export APIDNS_PUBLIC=$APIDNS_PUBLIC" >> /etc/profile

// API Server Endpoint 도메인의 IP 주소 확인
dig +short $APIDNS_PUBLIC
{% endhighlight %}

<br/>

<span style='color:white; background-color:#404040'> **EKS 노드 그룹 확인** </span>  
{% highlight javascript linenos %}
// eksctl을 통한 노드 그룹 정보 확인
eksctl get nodegroup --cluster $CLUSTER_NAME --name ${CLUSTER_NAME}-node-group

// awscli를 통한 노드 그룹 정보 확인 (상세)
aws eks describe-nodegroup --cluster-name $CLUSTER_NAME --nodegroup-name ${CLUSTER_NAME}-node-group | jq

// kubectl을 통한 노드 정보 확인
kubectl get node
kubectl get node -owide
kubectl get node -v=6
{% endhighlight %}

<br/>

<span style='color:white; background-color:#404040'> **노드 IP 변수 선언 및 SSH 접근** </span>  
{% highlight javascript linenos %}
// 노드 IP 변수 저장
PublicN1=$(kubectl get node --label-columns=topology.kubernetes.io/zone --selector=topology.kubernetes.io/zone=ap-northeast-2a -o jsonpath={.items[0].status.addresses[0].address})
PublicN2=$(kubectl get node --label-columns=topology.kubernetes.io/zone --selector=topology.kubernetes.io/zone=ap-northeast-2b -o jsonpath={.items[0].status.addresses[0].address})
PublicN3=$(kubectl get node --label-columns=topology.kubernetes.io/zone --selector=topology.kubernetes.io/zone=ap-northeast-2c -o jsonpath={.items[0].status.addresses[0].address})
echo "export PublicN1=$PublicN1" >> /etc/profile
echo "export PublicN2=$PublicN2" >> /etc/profile
echo "export PublicN3=$PublicN3" >> /etc/profile
echo $PublicN1, $PublicN2, $PublicN3

// 노드에 ssh 접근 확인
for node in $PublicN1 $PublicN2 $PublicN3; do ssh -i ~/.ssh/kp_node.pem -o StrictHostKeyChecking=no ec2-user@$node hostname; done
{% endhighlight %}

<br/>

<span style='color:white; background-color:#404040'> **실습에 활용할 도구 설치** </span>  
{% highlight javascript linenos %}
// AWS Load Balancer Controller 설치
helm repo add eks https://aws.github.io/eks-charts
helm repo update
helm install aws-load-balancer-controller eks/aws-load-balancer-controller -n kube-system --set clusterName=$CLUSTER_NAME \
  --set serviceAccount.create=false \
  --set serviceAccount.name=aws-load-balancer-controller

// ExternalDNS 설치
MyDomain=<자신의 도메인>
MyDnsHostedZoneId=$(aws route53 list-hosted-zones-by-name --dns-name "${MyDomain}." --query "HostedZones[0].Id" --output text)
echo $MyDomain, $MyDnsHostedZoneId
curl -s -O https://raw.githubusercontent.com/cloudneta/cnaeblab/master/_data/externaldns.yaml
MyDomain=$MyDomain MyDnsHostedZoneId=$MyDnsHostedZoneId envsubst < externaldns.yaml | kubectl apply -f -

// kube-ops-view 설치
helm repo add geek-cookbook https://geek-cookbook.github.io/charts/
helm install kube-ops-view geek-cookbook/kube-ops-view --version 1.2.2 --set env.TZ="Asia/Seoul" --namespace kube-system
kubectl patch svc -n kube-system kube-ops-view -p '{"spec":{"type":"LoadBalancer"}}'
kubectl annotate service kube-ops-view -n kube-system "external-dns.alpha.kubernetes.io/hostname=kubeopsview.$MyDomain"
echo -e "Kube Ops View URL = http://kubeopsview.$MyDomain:8080/#scale=1.5"
{% endhighlight %}

<br/><br/>

### 1.3. API Server - Endpoint Public Access 통신 흐름
Terraform 배포로 생성된 Amazon EKS Cluster는 Endpoint Public Access 환경으로 구성됩니다. 이러한 환경에서 3가지 측면으로 통신 흐름을 확인합니다.

<br/>

<span style='color:white; background-color:#404040'> **api-server에서 kubelet으로 통신** </span>

myeks-host에서 `kubectl exec` 명령으로 특정 파드에 접근합니다.  

{% highlight javascript linenos %}
// [터미널1] kube-ops-view deployments의 파드에 bash 실행
kubectl exec -it deploy/kube-ops-view -n kube-system -- bash

// [터미널2] exec 실행으로 tcp 세션 확인 시 Peer Address 확인
for i in $PublicN1 $PublicN2 $PublicN3; do echo ">> node $i <<"; ssh -i ~/.ssh/kp_node.pem ec2-user@$i sudo ss -tnp; echo; done
{% endhighlight %}

{: .box-note}
**Note:** kubelet에 연결된 Peer Address는 어떤 대상의 IP 주소일까요?

<br/>

<span style='color:white; background-color:#404040'> **myeks-host에서 api-server로 통신** </span>

myeks-host에서 `kubectl` 명령으로 api-server로 통신합니다.  

{% highlight javascript linenos %}
// [터미널1] tcpdump: tcp 443 (3Way Handshake)
tcpdump -n -i eth0 tcp port 443 -c 3

// [터미널2] kubectl 명령 수행
kubectl get node

// [터미널2] dig 명령으로 api-server endpoint 주소 확인
dig +short $APIDNS_PUBLIC
{% endhighlight %}

{: .box-note}
**Note:** myeks-host에서 api-server로 통신 흐름을 파악해 봅니다.

<br/>

<span style='color:white; background-color:#404040'> **kebelet과 kube-proxy에서 api-server로 통신** </span>

노드에 접근해서 TCP 세션 정보를 확인합니다.  

{% highlight javascript linenos %}
// 노드마다 tcp 세션 정보 확인
for i in $PublicN1 $PublicN2 $PublicN3; do echo ">> node $i <<"; ssh -i ~/.ssh/kp_node.pem ec2-user@$i sudo ss -tnp | grep -E "State|kube"; echo; done
{% endhighlight %}

{: .box-note}
**Note:** kubelet과 kube-proxy에서 api-server로 통신 흐름을 파악해 봅니다.

<br/>


---

<br/>

## 2. EKS Fully Private Cluster 배포 및 확인

<br/>

### 2.1. EKS Fully Private Cluster 배포

Amazon EKS Fully Private Cluster를 생성하기 위해 eksctl 명령을 활용해 배포합니다.  

<br/>

<span style='color:white; background-color:#404040'> **eksctl로 fully private cluster 생성** </span>

{% highlight javascript linenos %}
// eks-private-cluster.yaml 파일 생성
cat > eks-private-cluster.yaml <<EOF
---
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig
metadata:
  name: ${CLUSTER_NAME}-Private
  region: ${AWS_DEFAULT_REGION}
  version: "1.29"
privateCluster:
  enabled: true

vpc:
  id: "${VPCID}"
  subnets: 
    private:
      private-ap-northeast-2a:
        id: "${PrivateSubnet1}"
      private-ap-northeast-2b:
        id: "${PrivateSubnet2}"
      private-ap-northeast-2c:
        id: "${PrivateSubnet3}"
managedNodeGroups: 
- name: ${CLUSTER_NAME}-Private-ng
  instanceType: t3.medium
  minSize: 3
  desiredCapacity: 3
  maxSize: 3
  privateNetworking: true
  volumeSize: 50
  volumeType: gp3
  iam:
    withAddonPolicies:
      autoScaler: false
      albIngress: true
      cloudWatch: false
      externalDNS: true
  ssh:
      allow: true
      publicKeyPath: ~/.ssh/id_rsa.pub
  subnets:
    - private-ap-northeast-2a
    - private-ap-northeast-2b
    - private-ap-northeast-2c
addons:
- name: vpc-cni
  attachPolicyARNs:
    - arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy
- name: coredns
  version: latest
- name: kube-proxy
  version: latest
iam:
  withOIDC: true
  serviceAccounts: 
  - metadata:
      name: aws-load-balancer-controller
      namespace: kube-system
    wellKnownPolicies:
      awsLoadBalancerController: true
EOF

// [eksctl] EKS Fully Private Cluster 배포
eksctl create cluster -f eks-private-cluster.yaml
{% endhighlight %}

{: .box-note}
**Note:** 참고로 eksctl로 private access를 구성하면 바로 private 환경으로 구성되지 않고 public & private 환경으로 설정 후 자동으로 public access를 제거하는 업데이트를 수행합니다.

{: .box-warning}
**Warning:** EKS Fully Cluster 구성을 할 때 보안 그룹 설정이 필요합니다. 클러스터 배포가 진행 중일 때 빠른 시점에 아래 보안 그룹 생성 작업을 진행해 주세요. 너무 늦게 설정하면 클러스터 배포가 실패될 것입니다!!

<br/>

<span style='color:white; background-color:#404040'> **보안 그룹 인바운드 규칙 확인 및 추가** </span>

{% highlight javascript linenos %}
// myeks-Private Cluster의 보안 그룹 정보
aws ec2 describe-security-groups --query 'SecurityGroups[*].[GroupId, GroupName]' --output text | grep myeks-Private

// ClusterSharedNodeSG와 ControlPlaneSG의 보안 그룹 ID 확인 및 변수 선언
aws ec2 describe-security-groups --filters Name=group-name,Values=*myeks-Private-cluster-Cluster* --query "SecurityGroups[*].[GroupId]" --output text
aws ec2 describe-security-groups --filters Name=group-name,Values=*myeks-Private-cluster-Control* --query "SecurityGroups[*].[GroupId]" --output text
CSN_SGID=$(aws ec2 describe-security-groups --filters Name=group-name,Values=*myeks-Private-cluster-Cluster* --query "SecurityGroups[*].[GroupId]" --output text); echo $CSN_SGID
CP_SGID=$(aws ec2 describe-security-groups --filters Name=group-name,Values=*myeks-Private-cluster-Control* --query "SecurityGroups[*].[GroupId]" --output text); echo $CP_SGID

// ClusterSharedNodeSG와 ControlPlaneSG의 보안 그룹 규칙 확인
aws ec2 describe-security-groups --group-ids $CP_SGID --output yaml | yh
aws ec2 describe-security-groups --group-ids $CSN_SGID --output yaml | yh

// ClusterSharedNodeSG의 인바운드 규칙에 192.168.1.100/32 (myeks-host), TCP 443 Port 추가
aws ec2 authorize-security-group-ingress --group-id $CSN_SGID --protocol tcp --port 443 --cidr 192.168.1.100/32

// ControlPlaneSG의 인바운드 규칙에 192.168.1.100/32 (myeks-host), TCP 443 Port 추가
aws ec2 authorize-security-group-ingress --group-id $CP_SGID --protocol tcp --port 443 --cidr 192.168.1.100/32
{% endhighlight %}

<br/>

{: .box-note}
**Note:** myeks-Private 클러스터를 생성하는 과정에 관리 콘솔에 접속해서 정보를 살펴 봅니다. (EKS Cluster, 보안 그룹, VPC Endpoint)

<br/><br/>

### 2.2. EKS Fully Private Cluster 정보 확인 및 설정

<br/>

<span style='color:white; background-color:#404040'> **kubectx와 kubens 확인 및 설정** </span>

{% highlight javascript linenos %}
// kubectx 확인
kubectx

// kubens default 설정
kubens default
{% endhighlight %}

<br/>

<span style='color:white; background-color:#404040'> **EKS Private Cluster 정보 확인** </span>  
{% highlight javascript linenos %}
// kubectl을 통한 eks private cluster 정보 확인
kubectl cluster-info

// eksctl을 통한 eks private cluster 정보 확인
eksctl get cluster

// awscli를 통한 eks private cluster 정보 확인 (상세)
aws eks describe-cluster --name ${CLUSTER_NAME}-Private | jq

// awscli를 통한 eks private cluster 정보 확인 (endpoint 주소와 endpoint access 확인)
aws eks describe-cluster --name ${CLUSTER_NAME}-Private | grep endpoint
{% endhighlight %}

<br/>

<span style='color:white; background-color:#404040'> **API Server Endpoint 주소 확인 및 조회** </span>  
{% highlight javascript linenos %}
// API Server Endpoint 도메인 변수 선언
APIDNS_PRIVATE=$(aws eks describe-cluster --name ${CLUSTER_NAME}-Private | jq -r .cluster.endpoint | cut -d '/' -f 3)
echo $APIDNS_PRIVATE
echo "export APIDNS_PRIVATE=$APIDNS_PRIVATE" >> /etc/profile

// API Server Endpoint 도메인의 IP 주소 확인
dig +short $APIDNS_PRIVATE
{% endhighlight %}

<br/>

<span style='color:white; background-color:#404040'> **EKS 노드 그룹 확인** </span>  
{% highlight javascript linenos %}
// eksctl을 통한 노드 그룹 정보 확인
eksctl get nodegroup --cluster ${CLUSTER_NAME}-Private --name ${CLUSTER_NAME}-Private-ng

// awscli를 통한 노드 그룹 정보 확인 (상세)
aws eks describe-nodegroup --cluster-name ${CLUSTER_NAME}-Private --nodegroup-name ${CLUSTER_NAME}-Private-ng | jq

// kubectl을 통한 노드 정보 확인
kubectl get node
kubectl get node -owide
kubectl get node -v=6
{% endhighlight %}

<br/>

<span style='color:white; background-color:#404040'> **노드 IP 변수 선언 및 SSH 접근** </span>  
{% highlight javascript linenos %}
// 노드 IP 변수 저장
PrivateN1=$(kubectl get node --label-columns=topology.kubernetes.io/zone --selector=topology.kubernetes.io/zone=ap-northeast-2a -o jsonpath={.items[0].status.addresses[0].address})
PrivateN2=$(kubectl get node --label-columns=topology.kubernetes.io/zone --selector=topology.kubernetes.io/zone=ap-northeast-2b -o jsonpath={.items[0].status.addresses[0].address})
PrivateN3=$(kubectl get node --label-columns=topology.kubernetes.io/zone --selector=topology.kubernetes.io/zone=ap-northeast-2c -o jsonpath={.items[0].status.addresses[0].address})
echo "export PrivateN1=$PrivateN1" >> /etc/profile
echo "export PrivateN2=$PrivateN2" >> /etc/profile
echo "export PrivateN3=$PrivateN3" >> /etc/profile
echo $PrivateN1, $PrivateN2, $PrivateN3

// 노드에 ssh 접근 확인
for node in $PrivateN1 $PrivateN2 $PrivateN3; do ssh -o StrictHostKeyChecking=no ec2-user@$node hostname; done
{% endhighlight %}

<br/>

<span style='color:white; background-color:#404040'> **실습에 활용할 도구 설치** </span>  
{% highlight javascript linenos %}
// AWS Load Balancer Controller 설치
helm repo add eks https://aws.github.io/eks-charts
helm repo update
helm install aws-load-balancer-controller eks/aws-load-balancer-controller -n kube-system --set clusterName=${CLUSTER_NAME}-Private \
  --set serviceAccount.create=false \
  --set serviceAccount.name=aws-load-balancer-controller

// ExternalDNS 설치
MyDomain=<자신의 도메인>
MyDnsHostedZoneId=$(aws route53 list-hosted-zones-by-name --dns-name "${MyDomain}." --query "HostedZones[0].Id" --output text)
echo $MyDomain, $MyDnsHostedZoneId
curl -s -O https://raw.githubusercontent.com/cloudneta/cnaeblab/master/_data/externaldns.yaml
MyDomain=$MyDomain MyDnsHostedZoneId=$MyDnsHostedZoneId envsubst < externaldns.yaml | kubectl apply -f -

// kube-ops-view 설치
helm repo add geek-cookbook https://geek-cookbook.github.io/charts/
helm install kube-ops-view geek-cookbook/kube-ops-view --version 1.2.2 --set env.TZ="Asia/Seoul" --namespace kube-system
kubectl patch svc -n kube-system kube-ops-view -p '{"spec":{"type":"LoadBalancer"}}'
kubectl annotate service kube-ops-view -n kube-system "external-dns.alpha.kubernetes.io/hostname=kubeopsview.$MyDomain"
echo -e "Kube Ops View URL = http://kubeopsview.$MyDomain:8080/#scale=1.5"
{% endhighlight %}

<br/><br/>



### 2.3. API Server - Endpoint Private Access 통신 흐름

eksctl 배포로 생성된 Amazon EKS Cluster는 Endpoint Private Access 환경으로 구성됩니다. 이러한 환경에서 3가지 측면으로 통신 흐름을 확인합니다.  

<br/>

<span style='color:white; background-color:#404040'> **api-server에서 kubelet으로 통신** </span>

myeks-host에서 `kubectl exec` 명령으로 특정 파드에 접근합니다.  

{% highlight javascript linenos %}
// [터미널1] aws-node daemonsets의 파드에 bash 실행
kubectl exec daemonsets/aws-node -it -n kube-system -c aws-eks-nodeagent -- bash

// [터미널2] exec 실행으로 tcp 세션 확인 시 Peer Address 확인
for i in $PrivateN1 $PrivateN2 $PrivateN3; do echo ">> node $i <<"; ssh ec2-user@$i sudo ss -tnp; echo; done
{% endhighlight %}

{: .box-note}
**Note:** kubelet에 연결된 Peer Address는 어떤 대상의 IP 주소일까요?

<br/>

<span style='color:white; background-color:#404040'> **myeks-host에서 api-server로 통신** </span>

myeks-host에서 `kubectl` 명령으로 api-server로 통신합니다.  

{% highlight javascript linenos %}
// [터미널1] tcpdump: tcp 443 (3Way Handshake)
tcpdump -n -i eth0 tcp port 443 -c 3

// [터미널2] kubectl 명령 수행
kubectl get node

// [터미널2] dig 명령으로 api-server endpoint 주소 확인
dig +short $APIDNS_PRIVATE
{% endhighlight %}

{: .box-note}
**Note:** myeks-host에서 api-server로 통신 흐름을 파악해 봅니다.

<br/>

<span style='color:white; background-color:#404040'> **kebelet과 kube-proxy에서 api-server로 통신** </span>

노드에 접근해서 TCP 세션 정보를 확인합니다.  

{% highlight javascript linenos %}
// 노드마다 tcp 세션 정보 확인
for i in $PrivateN1 $PrivateN2 $PrivateN3; do echo ">> node $i <<"; ssh ec2-user@$i sudo ss -tnp | grep -E "State|kube"; echo; done
{% endhighlight %}

{: .box-note}
**Note:** kubelet과 kube-proxy에서 api-server로 통신 흐름을 파악해 봅니다.

<br/><br/>


### 2.4. 파드 구성 및 통신 확인

EKS Private Cluster 환경에서 파드를 구성하고 앞단에 Ingress ALB를 구성합니다.  

<br/>

<span style='color:white; background-color:#404040'> **vpc endpoint - elasticloadbalancing 생성** </span>

{% highlight javascript linenos %}
// [터미널1] vpc endpoint - elasticloadbalancing 생성
CSN_SGID=$(aws ec2 describe-security-groups --filters Name=group-name,Values=*myeks-Private-cluster-Cluster* --query "SecurityGroups[*].[GroupId]" --output text); echo $CSN_SGID

aws ec2 create-vpc-endpoint --vpc-id $VPCID --service-name com.amazonaws.ap-northeast-2.elasticloadbalancing --vpc-endpoint-type Interface --subnet-ids $PrivateSubnet1 $PrivateSubnet2 $PrivateSubnet3 --security-group-ids $CSN_SGID

// [터미널2] vpc endpoint의 SericeName과 State 모니터링
watch -d "aws ec2 describe-vpc-endpoints --query 'VpcEndpoints[*].[VpcEndpointId, ServiceName, State]' --output text"
{% endhighlight %}

{: .box-note}
**Note:** EKS Private Cluster 환경에서 ELB를 구성하려면 VPC Endpoint - Elastic Load Balancing 생성해야 합니다.

<br/>

<span style='color:white; background-color:#404040'> **파드와 Ingress ALB 생성** </span>

{% highlight javascript linenos %}
// [터미널2] 모니터링
watch -d kubectl get pod,ingress,svc,ep -n game-2048


// [터미널1] 파드와 Ingress ALB 생성
curl -s -O https://raw.githubusercontent.com/cloudneta/cnaeblab/master/_data/ingress1.yaml

cat ingress1.yaml | yh

kubectl apply -f ingress1.yaml


// [터미널3] elb 모니터링
watch -d "aws elbv2 describe-load-balancers --output text --query 'LoadBalancers[*].[LoadBalancerName, State.Code]'"


// [터미널1] externalDNS 연결
kubectl annotate ingress ingress-2048 -n game-2048 "external-dns.alpha.kubernetes.io/hostname=ingress.$MyDomain"
echo -e "2048 Game URL = http://ingress.$MyDomain"
{% endhighlight %}

<br/>

---

<br/>

## 3. 실습 환경 삭제

<br/>

### 3.1. EKS Fully Private Cluster 삭제

<br/>

<span style='color:white; background-color:#404040'> **EKS Fully Private Cluster 삭제** </span>

{% highlight javascript linenos %}
// 파드와 Ingress ALB 삭제
kubectl delete -f ingress1.yaml

// vpc endpoint - elasticloadbalancing 삭제
ELB_EPID=$(aws ec2 describe-vpc-endpoints | jq -r '.VpcEndpoints[] | select(.ServiceName=="com.amazonaws.ap-northeast-2.elasticloadbalancing") | .VpcEndpointId'); echo $ELB_EPID

aws ec2 delete-vpc-endpoints --vpc-endpoint-ids $ELB_EPID

// [터미널2] vpc endpoint의 SericeName과 State 모니터링
watch -d "aws ec2 describe-vpc-endpoints --query 'VpcEndpoints[*].[VpcEndpointId, ServiceName, State]' --output text"

// kube-ops-view 삭제
helm uninstall kube-ops-view -n kube-system

// myeks-Private Cluster 삭제
eksctl delete cluster --name $CLUSTER_NAME-Private
{% endhighlight %}

<br/><br/>


### 3.2. Terraform 삭제 또는 유지

1장 두 번째 실습을 이어서 진행하려면 현재 생성 자원을 유지합니다.  
만약 이어서 진행하지 않을 경우 아래 토글 영역을 확장해서 Terraform 삭제 작업을 진행합니다.

<br/>

{: .box-warning}
**Warning:** Terraform 삭제를 진행하기 전에 꼭 EKS Public Cluster에 배포한 kube-ops-view 먼저 삭제 후 진행해 주세요.

<details>
<summary><span style='color:orange'>Terraform 삭제</span></summary>
<div markdown="1">
<br/>

<span style='color:white; background-color:#404040'> **kube-ops-view 삭제** </span>

{% highlight javascript linenos %}
// context 변경
kubectx
kubectx [context name]

// kube-ops-view 삭제
helm uninstall kube-ops-view -n kube-system
{% endhighlight %}

<br/>

<span style='color:white; background-color:#404040'> **Terraform 삭제** </span>

{% highlight javascript linenos %}
// context 변경
terraform destroy -auto-approve
{% endhighlight %}

<br/>

</div>
</details>

<br/>


{: .box-warning}
**Warning:** Terraform 삭제 동안 터미널을 유지하고 Terraform 삭제가 완료되면 정상적으로 자원 삭제 되었는지 꼭 확인을 합니다.

<br/>

---

<br/>

여기까지 1장의 첫 번째 실습인 Amazon EKS Fully Private Cluster 실습을 마칩니다.  
수고하셨습니다 :)

<br/><br/>
